{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {
    "id": "44281064"
   },
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {
    "id": "221ffe46"
   },
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {
    "id": "7385ce53"
   },
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ac481e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ac481e",
    "outputId": "2b0053d1-0cbc-44b5-cb6a-da0a014293e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Sayuri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import unidecode\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import contractions\n",
    "import string\n",
    "from tqdm.notebook import tqdm # for showing progress bar\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# initialization\n",
    "pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "punc_translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "num_translator = str.maketrans(string.digits, ' ' * len(string.digits))\n",
    "nlp = en_core_web_sm.load()\n",
    "stopwords = stopwords.words('english')\n",
    "custom_negation = ['rather', 'instead']\n",
    "    \n",
    "def pre_processing(dataset):\n",
    "    \n",
    "    to_return = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        filtered_data = []\n",
    "        \n",
    "        # (1) remove html tags\n",
    "        dataset[i] = BeautifulSoup(dataset[i]).get_text()\n",
    "        \n",
    "        # (2) remove urls\n",
    "        dataset[i] = re.sub(r'http\\S+', '', dataset[i])\n",
    "        dataset[i] = re.sub(r'www\\S+', '', dataset[i])\n",
    "        \n",
    "        # (3) remove email addresses\n",
    "        dataset[i] = re.sub(r'\\S*@\\S*\\s?', '', dataset[i])\n",
    "        \n",
    "        # (3) convert to lower case\n",
    "        dataset[i] = dataset[i].casefold()\n",
    "        \n",
    "        # (4) convert accented character\n",
    "        dataset[i] = unidecode.unidecode(dataset[i]) \n",
    "        \n",
    "        # (5) if there are >2 consecutive duplicated characters, convert to 2 consecutive duplicated characters\n",
    "        # e.g. finallllly --> finally\n",
    "        dataset[i] = pattern.sub(r\"\\1\\1\", dataset[i]) \n",
    "        \n",
    "        # (6) expand contractions\n",
    "        dataset[i] = contractions.fix(dataset[i])\n",
    "        \n",
    "        # (7) replace punctuation with space\n",
    "        dataset[i] = dataset[i].translate(punc_translator)\n",
    "        \n",
    "        # (8) replace numbers with space\n",
    "        dataset[i] = dataset[i].translate(num_translator)\n",
    "        \n",
    "        # (9) spacy tokenization\n",
    "        tokens = nlp(dataset[i])\n",
    "            \n",
    "        for token in tokens:\n",
    "            \n",
    "            # Lemmatisation\n",
    "            word = token.lemma_\n",
    "            \n",
    "            # filter out words that are:\n",
    "            # - stopwords\n",
    "            # - with length <= 2\n",
    "            # - demonstratives (e.g. this, that, those)\n",
    "            # - pronoun and proper nouns (e.g. names)\n",
    "            # - spaces\n",
    "            \n",
    "            names = [ent.text for ent in tokens if ent.ent_type_]\n",
    "            \n",
    "            if (word != \"-PRON-\") and (word !=\"-PROPN-\") and (word not in names) and (not token.is_space):\n",
    "               \n",
    "                #print(word)\n",
    "\n",
    "                if (token.dep_ == 'neg') or (word in custom_negation):\n",
    "                    filtered_data.append('_NEG_')\n",
    "                    continue\n",
    "                \n",
    "                # remove the word \"like\" when it is used as preposition\n",
    "                if (word == 'like' and token.dep_ == 'prep'):\n",
    "                    continue\n",
    "                \n",
    "                # remove stopwords\n",
    "                if (word in stopwords):\n",
    "                    continue\n",
    "\n",
    "                # remove words with len <= 2\n",
    "                elif (len(word) <= 2):\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    filtered_data.append(word)\n",
    "        \n",
    "        # join words\n",
    "        filtered_data = ' '.join(filtered_data)\n",
    "        \n",
    "        # Negation tagging\n",
    "        filtered_data = re.sub(r'_NEG_\\s', '_NEG_', filtered_data)\n",
    "        filtered_data = re.sub(r\"(_NEG_)\\1{1,}\", '_NEG_', filtered_data) # remove duplicated negation tagging\n",
    "        \n",
    "        to_return.append(filtered_data)\n",
    "        \n",
    "    return to_return\n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    \n",
    "    # Read the CSV files for training and test sets\n",
    "    train = pd.read_csv(train_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = np.array(train.review)\n",
    "    X_test = np.array(test.review)\n",
    "    \n",
    "    y_train = np.array(train.sentiment)\n",
    "    y_train[y_train=='positive'] = 1\n",
    "    y_train[y_train=='negative'] = -1\n",
    "    y_train = y_train.astype('int')\n",
    "    \n",
    "    y_test = np.array(test.sentiment)\n",
    "    y_test[y_test=='positive'] = 1\n",
    "    y_test[y_test=='negative'] = -1\n",
    "    y_test = y_test.astype('int')\n",
    "    \n",
    "    # Extract bag of words features\n",
    "    print(\"Train set: \")\n",
    "    print(\"Preprocessing progress: \")\n",
    "    X_train = pre_processing(X_train) \n",
    "    print(\"--Done--\\n\")\n",
    "    print(\"Test set: \")\n",
    "    print(\"Preprocessing progress: \")\n",
    "    X_test = pre_processing(X_test)\n",
    "    print('--Done--')\n",
    "    \n",
    "    return (X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc3af27",
   "metadata": {
    "id": "7fc3af27"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from collections import defaultdict\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, kernel='rbf', C=1.6058997806999291, gamma=0.8101577349324269):\n",
    "        \n",
    "        #implement initialisation\n",
    "        self.clf = svm.SVC()\n",
    "        self.kernel = kernel\n",
    "        \n",
    "        # regularization parameter\n",
    "        self.C = C # penalty parameter\n",
    "        \n",
    "        # kernel parameters\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(min_df = 2, # remove words that appear too rarely\n",
    "                                          max_df = 0.7, # remove words that appear too often\n",
    "                                          ngram_range=(1,5), # 1-2 gram\n",
    "                                          max_features=30000,\n",
    "                                          smooth_idf = True, # +1 to all frequencies, prevent division by zero\n",
    "                                          sublinear_tf = True #use log for TF, clip extreme values\n",
    "                                          )\n",
    "        \n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "  \n",
    "    def custom_kernel(self, X, y):\n",
    "        \n",
    "        X = X.A\n",
    "        y = y.A\n",
    "        \n",
    "        print('Computing custom kernel...')\n",
    "        \n",
    "        # 1. Histogram intersection kernel （0.86）\n",
    "        # ---------------- BEGIN ----------------#\n",
    "        kernel = np.zeros((X.shape[0], y.shape[0]))\n",
    "\n",
    "        for d in tqdm(range(X.shape[1])):\n",
    "            column_1 = X[:, d].reshape(-1, 1)\n",
    "            column_2 = y[:, d].reshape(-1, 1)\n",
    "            kernel += np.minimum(column_1, column_2.T)\n",
    "\n",
    "        # ------------------ END -----------------#\n",
    "        \n",
    "        return kernel\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "\n",
    "        # Transform data into tfidf feature vectors\n",
    "        X = self.vectorizer.fit_transform(X)\n",
    "\n",
    "        # calling diff kernels\n",
    "        if self.kernel == 'linear':\n",
    "            self.clf = svm.SVC(kernel='linear', C=self.C)\n",
    "\n",
    "        elif self.kernel == 'poly':\n",
    "            self.clf = svm.SVC(kernel='poly', C=self.C, degree=self.d)\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            # for hyperparameter tuning\n",
    "            self.clf = svm.SVC(kernel='rbf', C=self.C, gamma=self.gamma)\n",
    "\n",
    "        elif self.kernel == 'custom':\n",
    "            self.clf = svm.SVC(kernel=self.custom_kernel, C=self.C)\n",
    "        \n",
    "        self.clf.fit(X,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # prediction routine for the SVM\n",
    "        X = self.vectorizer.transform(X)\n",
    "        \n",
    "        return self.clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {
    "id": "35e6f272"
   },
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89603f43",
   "metadata": {
    "id": "89603f43"
   },
   "outputs": [],
   "source": [
    "def test_func_svm(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ffd4adf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "4ffd4adf",
    "outputId": "a240432a-90d9-4499-a2c5-3121891d8938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: \n",
      "Preprocessing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b799f1fd7404ad9b240acd86a076d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Done--\n",
      "\n",
      "Test set: \n",
      "Preprocessing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a57a3402d1e4ce386950614a50c4267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Done--\n",
      "Accuracy: 0.888\n"
     ]
    }
   ],
   "source": [
    "acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {
    "id": "61056292"
   },
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3805e672",
   "metadata": {
    "id": "3805e672"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    # n_clf: number of trees to be trained\n",
    "    # max_depth: the depth of the trees to be trained\n",
    "    # criterion: how the trees calculate the splitting point\n",
    "    def __init__(self,n_clf=10000, max_depth=1, criterion=\"gini\", splitter=\"best\"):\n",
    "        \n",
    "        # Hyperparameter for AdaBoost\n",
    "        # The number of trees to be trained and averaged\n",
    "        self.n_clf=n_clf\n",
    "        \n",
    "        # Hyperparameters for decision tree\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        \n",
    "        # TF-IDF vectorizer to convert the feature vectors\n",
    "        self.tf_idf = TfidfVectorizer(min_df = 2, # remove words that appear too rarely\n",
    "                                      max_df = 0.7, # remove words that appear too often\n",
    "                                      sublinear_tf = True,\n",
    "                                      ngram_range=(1,5), # how many size+number of ngrams the data is split into\n",
    "                                      max_features=30000, # Limit the size of feature vector to save time\n",
    "                                      smooth_idf = True\n",
    "                                      )\n",
    "        \n",
    "\n",
    "\n",
    "    def update_w(self, w, al, y, pred):\n",
    "        # W is the weights of each data point in the data set\n",
    "        # Misclassified points have their weight raised\n",
    "        return w * np.exp(al * (np.not_equal(y, pred)))\n",
    "    \n",
    "    def calc_err(self, y, pred, w):\n",
    "        # Error is given by the misclassified points multiplied by their respective weights.\n",
    "        return sum(w * np.not_equal(y, pred))/sum(w)\n",
    "    \n",
    "    def calc_alph(self, err):\n",
    "        # Alpha is a function of the error\n",
    "        # Low error classifiers will receive higher value of alpha\n",
    "        # i.e. higher weight when weak-learners are averaged.\n",
    "        return np.log((1 - err) / err )\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        n_samples = len(X)\n",
    "        self.clfs=[]\n",
    "        self.alpha=[]\n",
    "\n",
    "        X = self.tf_idf.fit_transform(X)\n",
    "        \n",
    "        for m in tqdm(range(self.n_clf)):\n",
    "            \n",
    "            if m == 0:\n",
    "                # init weights\n",
    "                w = np.full(n_samples,(1/n_samples))\n",
    "            else:\n",
    "                # update weights\n",
    "                w = self.update_w(w, alph, y, pred)\n",
    "            \n",
    "            # Build a weak learner\n",
    "            clf = DecisionTreeClassifier(max_depth=self.max_depth,\n",
    "                                         criterion = self.criterion,\n",
    "                                         splitter = self.splitter\n",
    "                                        )\n",
    "            # Fit the weak classifier\n",
    "            clf = clf.fit(X, y, sample_weight=w)\n",
    "            \n",
    "            pred = clf.predict(X) # predictions made by the weak classifier\n",
    "            \n",
    "            # save classifier\n",
    "            self.clfs.append(clf)\n",
    "            \n",
    "            # calculate error\n",
    "            err = self.calc_err(y, pred, w)\n",
    "            \n",
    "            # cal alph \n",
    "            alph = self.calc_alph(err)\n",
    "            self.alpha.append(alph)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # init df for storing pred from each weak classifier (decision tree)\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.n_clf))\n",
    "        \n",
    "        X = self.tf_idf.transform(X)    \n",
    "        \n",
    "        for m in tqdm(range(self.n_clf)):\n",
    "            pred_m = self.clfs[m].predict(X) * self.alpha[m]\n",
    "            weak_preds.iloc[:,m] = pred_m\n",
    "\n",
    "        # Calculate final predictions\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {
    "id": "af6e0987"
   },
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4632591c",
   "metadata": {
    "id": "4632591c"
   },
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    Y_Pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c27de1",
   "metadata": {
    "id": "d6c27de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: \n",
      "Preprocessing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837782c327e54b3ba6c2e88ebd6dbe22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Done--\n",
      "\n",
      "Test set: \n",
      "Preprocessing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3351d7423744d695ad9c75505d730b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Done--\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625a4480d1a447508d81be958c4b211c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0cfba8e12340d68204f9f81695ff85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Coursework2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
