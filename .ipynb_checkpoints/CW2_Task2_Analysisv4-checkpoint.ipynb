{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6fcec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\lolon\\appdata\\roaming\\python\\python38\\site-packages (0.17.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for spacy: [Errno 2] No such file or directory: 'c:\\\\users\\\\lolon\\\\anaconda3\\\\lib\\\\site-packages\\\\spacy-3.2.4.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\lolon\\appdata\\roaming\\python\\python38\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\lolon\\appdata\\roaming\\python\\python38\\site-packages (3.8.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for spacy: [Errno 2] No such file or directory: 'c:\\\\users\\\\lolon\\\\anaconda3\\\\lib\\\\site-packages\\\\spacy-3.2.4.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from gensim) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\lolon\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.5.4)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\lolon\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: testfixtures in c:\\users\\lolon\\anaconda3\\lib\\site-packages (6.18.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for spacy: [Errno 2] No such file or directory: 'c:\\\\users\\\\lolon\\\\anaconda3\\\\lib\\\\site-packages\\\\spacy-3.2.4.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting negspacy\n",
      "  Using cached negspacy-1.0.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.0.1 in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from negspacy) (3.2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\lolon\\\\anaconda3\\\\lib\\\\site-packages\\\\spacy-3.2.4.dist-info\\\\METADATA'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "     --------------------------------------- 12.0/12.0 MB 14.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: spacy>=2.2.0 in c:\\users\\lolon\\anaconda3\\lib\\site-packages (from en-core-web-sm==2.2.0) (3.2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\lolon\\\\anaconda3\\\\lib\\\\site-packages\\\\spacy-3.2.4.dist-info\\\\METADATA'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\lolon\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textblob\n",
    "!pip3 install gensim\n",
    "!pip3 install testfixtures\n",
    "!pip3 install negspacy\n",
    "!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lolon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\lolon\\anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import unidecode\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import contractions\n",
    "import string\n",
    "from tqdm.notebook import tqdm # for showing progress bar\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# initialization\n",
    "pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "punc_translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "num_translator = str.maketrans(string.digits, ' ' * len(string.digits))\n",
    "nlp = en_core_web_sm.load()\n",
    "stopwords = stopwords.words('english')\n",
    "custom_negation = ['rather', 'instead']\n",
    "    \n",
    "def pre_processing(dataset):\n",
    "    \n",
    "    to_return = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        filtered_data = []\n",
    "        \n",
    "        # (1) remove html tags\n",
    "        dataset[i] = BeautifulSoup(dataset[i]).get_text()\n",
    "        \n",
    "        # (2) remove urls\n",
    "        dataset[i] = re.sub(r'http\\S+', '', dataset[i])\n",
    "        dataset[i] = re.sub(r'www\\S+', '', dataset[i])\n",
    "        \n",
    "        # (3) remove email addresses\n",
    "        dataset[i] = re.sub(r'\\S*@\\S*\\s?', '', dataset[i])\n",
    "        \n",
    "        # (3) convert to lower case\n",
    "        dataset[i] = dataset[i].casefold()\n",
    "        \n",
    "        # (4) convert accented character\n",
    "        dataset[i] = unidecode.unidecode(dataset[i]) \n",
    "        \n",
    "        # (5) if there are >2 consecutive duplicated characters, convert to 2 consecutive duplicated characters\n",
    "        # e.g. finallllly --> finally\n",
    "        dataset[i] = pattern.sub(r\"\\1\\1\", dataset[i]) \n",
    "        \n",
    "        # (6) expand contractions\n",
    "        dataset[i] = contractions.fix(dataset[i])\n",
    "        \n",
    "        # (7) replace punctuation with space\n",
    "        dataset[i] = dataset[i].translate(punc_translator)\n",
    "        \n",
    "        # (8) replace numbers with space\n",
    "        dataset[i] = dataset[i].translate(num_translator)\n",
    "        \n",
    "        # (9) spacy tokenization\n",
    "        tokens = nlp(dataset[i])\n",
    "            \n",
    "        for token in tokens:\n",
    "            \n",
    "            # Lemmatisation\n",
    "            word = token.lemma_\n",
    "            \n",
    "            # filter out words that are:\n",
    "            # - stopwords\n",
    "            # - with length <= 2\n",
    "            # - demonstratives (e.g. this, that, those)\n",
    "            # - pronoun and proper nouns (e.g. names)\n",
    "            # - spaces\n",
    "            \n",
    "            names = [ent.text for ent in tokens if ent.ent_type_]\n",
    "            \n",
    "            if (word != \"-PRON-\") and (word !=\"-PROPN-\") and (word not in names) and (not token.is_space):\n",
    "               \n",
    "                #print(word)\n",
    "\n",
    "                if (token.dep_ == 'neg') or (word in custom_negation):\n",
    "                    filtered_data.append('_NEG_')\n",
    "                    continue\n",
    "                \n",
    "                # remove the word \"like\" when it is used as preposition\n",
    "                if (word == 'like' and token.dep_ == 'prep'):\n",
    "                    continue\n",
    "                \n",
    "                # remove stopwords\n",
    "                if (word in stopwords):\n",
    "                    continue\n",
    "\n",
    "                # remove words with len <= 2\n",
    "                elif (len(word) <= 2):\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    filtered_data.append(word)\n",
    "        \n",
    "        # join words\n",
    "        filtered_data = ' '.join(filtered_data)\n",
    "        \n",
    "        # Negation tagging\n",
    "        filtered_data = re.sub(r'_NEG_\\s', '_NEG_', filtered_data)\n",
    "        filtered_data = re.sub(r\"(_NEG_)\\1{1,}\", '_NEG_', filtered_data) # remove duplicated negation tagging\n",
    "        \n",
    "        to_return.append(filtered_data)\n",
    "        \n",
    "    return to_return\n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    \n",
    "    # Read the CSV files for training and test sets\n",
    "    train = pd.read_csv(train_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = np.array(train.review)\n",
    "    X_test = np.array(test.review)\n",
    "    \n",
    "    y_train = np.array(train.sentiment)\n",
    "    y_train[y_train=='positive'] = 1\n",
    "    y_train[y_train=='negative'] = -1\n",
    "    y_train = y_train.astype('int')\n",
    "    \n",
    "    y_test = np.array(test.sentiment)\n",
    "    y_test[y_test=='positive'] = 1\n",
    "    y_test[y_test=='negative'] = -1\n",
    "    y_test = y_test.astype('int')\n",
    "    \n",
    "    # Extract bag of words features\n",
    "    print(\"Train set: \")\n",
    "    print(\"Preprocessing progress: \")\n",
    "    X_train = pre_processing(X_train) \n",
    "    print(\"--Done--\\n\")\n",
    "    print(\"Test set: \")\n",
    "    print(\"Preprocessing progress: \")\n",
    "    X_test = pre_processing(X_test)\n",
    "    print('--Done--')\n",
    "    \n",
    "    return (X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26584cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: \n",
      "Preprocessing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ac85df572648b4986871613093daef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Done--\n",
      "\n",
      "Test set: \n",
      "Preprocessing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed34bf256eac40299be30108e6971b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Done--\n"
     ]
    }
   ],
   "source": [
    "# self testing code - remove before submission\n",
    "(X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3805e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    # n_clf: number of trees to be trained\n",
    "    # max_depth: the depth of the trees to be trained\n",
    "    # criterion: how the trees calculate the splitting point\n",
    "    def __init__(self,n_clf=10000, max_depth=1, criterion=\"gini\", splitter=\"best\"):\n",
    "        \n",
    "        # Hyperparameter for AdaBoost\n",
    "        # The number of trees to be trained and averaged\n",
    "        self.n_clf=n_clf\n",
    "        \n",
    "        # Hyperparameters for decision tree\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        \n",
    "        # TF-IDF vectorizer to convert the feature vectors\n",
    "        self.tf_idf = TfidfVectorizer(min_df = 2, # remove words that appear too rarely\n",
    "                                      max_df = 0.7, # remove words that appear too often\n",
    "                                      sublinear_tf = True,\n",
    "                                      ngram_range=(1,5), # how many size+number of ngrams the data is split into\n",
    "                                      max_features=30000, # Limit the size of feature vector to save time\n",
    "                                      smooth_idf = True\n",
    "                                      )\n",
    "        \n",
    "\n",
    "\n",
    "    def update_w(self, w, al, y, pred):\n",
    "        # W is the weights of each data point in the data set\n",
    "        # Misclassified points have their weight raised\n",
    "        return w * np.exp(al * (np.not_equal(y, pred)))\n",
    "    \n",
    "    def calc_err(self, y, pred, w):\n",
    "        # Error is given by the misclassified points multiplied by their respective weights.\n",
    "        return sum(w * np.not_equal(y, pred))/sum(w)\n",
    "    \n",
    "    def calc_alph(self, err):\n",
    "        # Alpha is a function of the error\n",
    "        # Low error classifiers will receive higher value of alpha\n",
    "        # i.e. higher weight when weak-learners are averaged.\n",
    "        return np.log((1 - err) / err )\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        n_samples = len(X)\n",
    "        self.clfs=[]\n",
    "        self.alpha=[]\n",
    "\n",
    "        X = self.tf_idf.fit_transform(X)\n",
    "        \n",
    "        for m in tqdm(range(self.n_clf)):\n",
    "            \n",
    "            if m == 0:\n",
    "                # init weights\n",
    "                w = np.full(n_samples,(1/n_samples))\n",
    "            else:\n",
    "                # update weights\n",
    "                w = self.update_w(w, alph, y, pred)\n",
    "            \n",
    "            # Build a weak learner\n",
    "            clf = DecisionTreeClassifier(max_depth=self.max_depth,\n",
    "                                         criterion = self.criterion,\n",
    "                                         splitter = self.splitter\n",
    "                                        )\n",
    "            # Fit the weak classifier\n",
    "            clf = clf.fit(X, y, sample_weight=w)\n",
    "            \n",
    "            pred = clf.predict(X) # predictions made by the weak classifier\n",
    "            \n",
    "            # save classifier\n",
    "            self.clfs.append(clf)\n",
    "            \n",
    "            # calculate error\n",
    "            err = self.calc_err(y, pred, w)\n",
    "            \n",
    "            # cal alph \n",
    "            alph = self.calc_alph(err)\n",
    "            self.alpha.append(alph)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # init df for storing pred from each weak classifier (decision tree)\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.n_clf))\n",
    "        \n",
    "        X = self.tf_idf.transform(X)    \n",
    "        \n",
    "        for m in tqdm(range(self.n_clf)):\n",
    "            pred_m = self.clfs[m].predict(X) * self.alpha[m]\n",
    "            weak_preds.iloc[:,m] = pred_m\n",
    "\n",
    "        # Calculate final predictions\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.countplot(Y_train).set(title='Training set')\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f688f",
   "metadata": {},
   "source": [
    "### Train + test using optimal hyperparameters found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f9cfeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b93e52401c40f1be3ddcb1420ea41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614e122195db4a0a8fc6b28a10e51371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25576/3906155113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0macc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy :\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mclassificationReport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# 5000 depth-1 trees for the best accuracy with reasonably quick runtime.\n",
    "clf=BoostingClassifier(n_clf=5000,  max_depth=1, criterion=\"gini\", splitter='best')\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# y_pred to hold the predictions of our classifier\n",
    "y_pred= clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(Y_test,y_pred)\n",
    "print(\"Accuracy :\",acc)\n",
    "classificationReport = classification_report(Y_test, y_pred)\n",
    "end = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338276b",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e2ece3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602766f",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a065aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X_train, Y_train)\n",
    "\n",
    "X = np.array(X_train)\n",
    "Y = np.array(Y_train)\n",
    "X_val = np.array(X_train)\n",
    "Y_val = np.array(Y_train)\n",
    "\n",
    "\n",
    "num_trees = [50,100,250,500]\n",
    "depths = [1,2,3,4,5]\n",
    "criteria = ['gini','entropy']\n",
    "\n",
    "\n",
    "fold = 1\n",
    "result = []\n",
    "\n",
    "for num_tree in num_trees:\n",
    "    for depth in depths:\n",
    "        for crit in criteria:\n",
    "            print('Number of trees: ', num_tree)\n",
    "            print('Depth:', depth)\n",
    "            print('Criteria:', crit)\n",
    "            fold = 1\n",
    "            reports = []\n",
    "            for train_idx, test_idx in skf.split(X, Y):\n",
    "\n",
    "                print('Fold:', fold)\n",
    "                clf = BoostingClassifier(n_clf=num_tree,  max_depth=depth, criterion=crit, splitter='best')\n",
    "                clf.fit(X[train_idx],Y[train_idx])\n",
    "                y_pred = clf.predict(X_val[test_idx])\n",
    "                report = classification_report(Y_val[test_idx], y_pred, output_dict=True)\n",
    "                reports.append(pd.DataFrame(report).transpose())\n",
    "                fold += 1\n",
    "\n",
    "            result.append(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44200b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X_train, Y_train)\n",
    "\n",
    "X = np.array(X_train)\n",
    "Y = np.array(Y_train)\n",
    "X_val = np.array(X_train)\n",
    "Y_val = np.array(Y_train)\n",
    "\n",
    "\n",
    "num_trees = [1000,5000,10000]\n",
    "depths = [1,2,3]\n",
    "\n",
    "fold = 1\n",
    "results = []\n",
    "\n",
    "for num_tree in num_trees:\n",
    "    for depth in depths:\n",
    "        print('Number of trees: ', num_tree)\n",
    "        print('Depth:', depth)\n",
    "        fold = 1\n",
    "        reports = []\n",
    "        for train_idx, test_idx in skf.split(X, Y):\n",
    "            \n",
    "            print('Fold:', fold)\n",
    "            clf = BoostingClassifier(n_clf=num_tree,  max_depth=depth, criterion='gini', splitter='best')\n",
    "            clf.fit(X[train_idx],Y[train_idx])\n",
    "            y_pred = clf.predict(X_val[test_idx])\n",
    "            report = classification_report(Y_val[test_idx], y_pred, output_dict=True)\n",
    "            reports.append(pd.DataFrame(report).transpose())\n",
    "            fold += 1\n",
    "        \n",
    "        results.append(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd0275c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 24.0, 'Tree'), Text(51.0, 0.5, 'Depth')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gini_10000 = {1000:[0.8294,0.8380,0.8398],5000:[0.8422,0.8426,0.8398],10000:[0.8404,0.8458,0.8414]}\n",
    "sns4 = pd.DataFrame(gini_10000,index=pd.RangeIndex(start=1, stop=4, name='index'))\n",
    "# use.corr()\n",
    "fig, ax = plt.subplots(figsize=(8,5))         # Sample figsize in inches\n",
    "sns.heatmap(sns4, ax=ax)\n",
    "ax.set(xlabel='Tree', ylabel='Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = {50:[0.7658,0.7564,0.7962],100:[0.7904,0.7824,0.813,],250:[0.8128,0.8086,0.8264],500:[0.8204,0.8154,0.8316],\n",
    "       1000:[0.8294,0.8380,0.8398],5000:[0.8422,0.8426,0.8398],10000:[0.8404,0.8458,0.8414]}\n",
    "sns1 = pd.DataFrame(overall,index=pd.RangeIndex(start=1, stop=4, name='index'))\n",
    "# use.corr()\n",
    "fig, ax = plt.subplots(figsize=(8,5))         # Sample figsize in inches\n",
    "sns.heatmap(sns1, ax=ax)\n",
    "ax.set(xlabel='Tree', ylabel='Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d64b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_500 = {50:[0.8018,0.8086,0.8042,0.8100,0.8056],100:[0.8158,0.8232,0.8222,0.8226,0.8166],\n",
    "            250:[0.8128,0.8086,0.8264,0.8162,0.8286],500:[0.8314,0.8384,0.8382,0.8316,0.8364]}\n",
    "gini_500_df = pd.DataFrame(gini_500,index=pd.RangeIndex(start=1, stop=6, name='index'))\n",
    "fig, ax = plt.subplots(figsize=(8,5))         # Sample figsize in inches\n",
    "sns.heatmap(gini_500_df, ax=ax)\n",
    "ax.set(xlabel='Tree', ylabel='Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_500 = {50:[0.7658,0.7564,0.7962,0.7888,0.7990],100:[0.7904,0.7824,0.813,0.8022,0.8162],\n",
    "            250:[0.8250,0.8298,0.8258,0.8268,0.8290],500:[0.8204,0.8154,0.8316,0.8274,0.8344]}\n",
    "entropy_500_df = pd.DataFrame(entropy_500,index=pd.RangeIndex(start=1, stop=6, name='index'))\n",
    "fig, ax = plt.subplots(figsize=(8,5))         # Sample figsize in inches\n",
    "sns.heatmap(entropy_500_df, ax=ax)\n",
    "ax.set(xlabel='Tree', ylabel='Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aa5845e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25576/2820114045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msum_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0msum_\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mavg_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum_\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "ac=list()\n",
    "pre=list()\n",
    "rec=list()\n",
    "fsco=list()\n",
    "for i in range(9):\n",
    "    sum_=0\n",
    "    for j in range(5):\n",
    "        sum_+=results[i][j].iloc[2][2]\n",
    "    avg_=sum_/5\n",
    "    ac.append(avg_)\n",
    "number = []\n",
    "depths = [1,2,3,4,5]\n",
    "for num_tree in num_trees:\n",
    "    for depth in depths:\n",
    "        number.append((num_tree,depth))\n",
    "acc_dict={}\n",
    "for i in range(9):\n",
    "    acc_dict[number[i]]=ac[i]\n",
    "max_=0\n",
    "for i in acc_dict.keys():\n",
    "    if acc_dict[i]>max_:\n",
    "        max_=acc_dict[i]\n",
    "        comb=i        \n",
    "print('------------------------------')\n",
    "print('Best accuracy :',max_)\n",
    "print('Best combinations :',comb)\n",
    "\n",
    "x_axis=['Tree: 1000,Max Depth: 1',\n",
    " 'Tree: 1000,Depth: 2',\n",
    " 'Tree: 1000,Depth: 3',\n",
    " 'Tree: 5000,Depth: 1',\n",
    " 'Tree: 5000,Depth: 2',\n",
    " 'Tree: 5000,Depth: 3',\n",
    " 'Tree: 10000,Depth: 1',\n",
    " 'Tree: 10000,Depth: 2',\n",
    " 'Tree: 10000,Depth: 3']\n",
    "x = x_axis\n",
    "y = list(acc_dict.values())\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title('Comparing Boosting with different hyperparameters')\n",
    "plt.xlabel('No of Trees & Max Depths')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(x,y,'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90610d",
   "metadata": {},
   "source": [
    "### Commonly Misclassified Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91c281d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"movie_review_train.csv\")\n",
    "test = pd.read_csv(\"movie_review_test.csv\")\n",
    "    \n",
    "Y_train = np.array(train.sentiment)\n",
    "Y_train[Y_train=='positive'] = 1\n",
    "Y_train[Y_train=='negative'] = -1\n",
    "Y_train = Y_train.astype('int')\n",
    "    \n",
    "Y_test = np.array(test.sentiment)\n",
    "Y_test[Y_test=='positive'] = 1\n",
    "Y_test[Y_test=='negative'] = -1\n",
    "Y_test = Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60656389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of misclassified words\n",
    "wrong_idx = [w for w in range(len(y_pred)) if y_pred[w] != Y_test[w]]\n",
    "\n",
    "# Indexes of correctly classified words\n",
    "correct_idx = [w for w in range(len(y_pred)) if y_pred[w] == Y_test[w]]\n",
    "\n",
    "# lists of the raw samples, one for correct and one for wrong.\n",
    "raw_wrong_samples = [test.review[i] for i in wrong_idx]\n",
    "raw_correct_samples = [test.review[i] for i in correct_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common words in misclassified samples\n",
    "# map the feature value\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Function that counts words in a list of reviews\n",
    "# and then sorts from most frequent to least.\n",
    "# This will be applied to our lists of correctly and incorrectly\n",
    "# classified reviews, so we can see the most frequent words in each.\n",
    "def sorted_count(reviews):\n",
    "    words = []\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        for word in tokens:\n",
    "            words.append(word)\n",
    "    unique, counts = np.unique(np.array(words), return_counts=True)\n",
    "    sorted_indices = np.argsort(counts)\n",
    "    sorted_unique = unique[sorted_indices]\n",
    "    sorted_counts = counts[sorted_indices]\n",
    "    \n",
    "    return sorted_unique, sorted_counts\n",
    "    \n",
    "fn_samples = [X_test[i] for i in wrong_idx if Y_test[i] == 1]\n",
    "fp_samples = [X_test[i] for i in wrong_idx if Y_test[i] == -1]\n",
    "\n",
    "fn_unique, fn_counts = sorted_count(fn_samples)\n",
    "fp_unique, fp_counts = sorted_count(fp_samples)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Most common words in false negative samples')\n",
    "plt.bar(fn_unique[-15:], fn_counts[-15:])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Most common words in false positive samples')\n",
    "plt.bar(fp_unique[-15:], fp_counts[-15:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "nlp = en_core_web_sm.load()\n",
    "test = pd.read_csv(r'movie_review_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19393c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for results analysis\n",
    "\n",
    "count = lambda data, to_count: sum([1 for x in data if x in to_count])\n",
    "\n",
    "# 1. Number of tokens\n",
    "def get_mean_len(X, Y, indices, sent):\n",
    "    samples = [X[i] for i in indices if Y[i] == sent]\n",
    "    samples = [nlp(review) for review in samples]\n",
    "    return sum(len(review) for review in samples) / len(samples)\n",
    "\n",
    "# 2. Number of punctuations\n",
    "def get_mean_punc(X, Y, indices, sent):\n",
    "    samples = [X[i] for i in indices if Y[i] == sent]\n",
    "    c = 0\n",
    "    ttl_len = 0\n",
    "    for i in range(len(samples)):\n",
    "        tokens = nlp(samples[i])\n",
    "        c += count(samples[i], set(string.punctuation))\n",
    "        ttl_len += len(tokens)\n",
    "    return (c / ttl_len) * 100\n",
    "\n",
    "# 3. Number of words trimmed\n",
    "def get_mean_trim(X, X_trim, Y, indices, sent):\n",
    "    samples = [X[i] for i in indices if Y[i] == sent]\n",
    "    trim_samples = [X_trim[i] for i in indices if Y[i] == sent]\n",
    "    return (sum(((len(samples[i].split()) - len(trim_samples[i].split()))/len(samples[i].split())) \\\n",
    "               for i in range(len(samples))) / len(samples))*100\n",
    "\n",
    "\n",
    "# 4. Number of adjectives\n",
    "def get_mean_pos(X, Y, indices, sent, pos):\n",
    "    samples = [X[i] for i in indices if Y[i] == sent]\n",
    "    count = 0\n",
    "    ttl_len = 0\n",
    "    for i in range(len(samples)):\n",
    "        tokens = nlp(samples[i])\n",
    "        count += len([token for token in tokens if token.pos_ == pos])\n",
    "        ttl_len += len(tokens)\n",
    "    return (count / ttl_len)*100\n",
    "    #return round(count/len(samples), 2)\n",
    "\n",
    "# 5. Number of negation words\n",
    "def get_mean_dep(X, Y, indices, sent, dep):\n",
    "    samples = [X[i] for i in indices if Y[i] == sent]\n",
    "    count = 0\n",
    "    ttl_len = 0\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        tokens = nlp(samples[i])\n",
    "        count += len([token for token in tokens if token.dep_ == dep])\n",
    "        ttl_len += len(tokens)\n",
    "        \n",
    "    return (count/ttl_len)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_pred = y_pred\n",
    "\n",
    "cus_wrong_idx = [w for w in range(len(cus_pred)) if cus_pred[w] != Y_test[w]]\n",
    "cus_correct_idx = [w for w in range(len(cus_pred)) if cus_pred[w] == Y_test[w]]\n",
    "\n",
    "cus_raw_wrong_samples = [test.review[i] for i in cus_wrong_idx]\n",
    "cus_raw_correct_samples = [test.review[i] for i in cus_correct_idx]\n",
    "\n",
    "# 1. Number of tokens\n",
    "cus_fn_mean_len = get_mean_len(X_test, Y_test, cus_wrong_idx, 1)\n",
    "cus_fp_mean_len = get_mean_len(X_test, Y_test, cus_wrong_idx, -1)\n",
    "cus_tn_mean_len = get_mean_len(X_test, Y_test, cus_correct_idx, -1)\n",
    "cus_tp_mean_len = get_mean_len(X_test, Y_test, cus_correct_idx, 1)\n",
    "\n",
    "# 2. Number of punctuations\n",
    "cus_fn_mean_punc = get_mean_punc(test.review, Y_test, cus_wrong_idx, 1)\n",
    "cus_fp_mean_punc = get_mean_punc(test.review, Y_test, cus_wrong_idx, -1)\n",
    "cus_tn_mean_punc = get_mean_punc(test.review, Y_test, cus_correct_idx, -1)\n",
    "cus_tp_mean_punc = get_mean_punc(test.review, Y_test, cus_correct_idx, 1)\n",
    "\n",
    "# 3. Number of negation words\n",
    "cus_fn_mean_neg = get_mean_dep(test.review, Y_test, cus_wrong_idx, 1, 'neg')\n",
    "cus_fp_mean_neg = get_mean_dep(test.review, Y_test, cus_wrong_idx, -1, 'neg')\n",
    "cus_tn_mean_neg = get_mean_dep(test.review, Y_test, cus_correct_idx, -1, 'neg')\n",
    "cus_tp_mean_neg = get_mean_dep(test.review, Y_test, cus_correct_idx, 1, 'neg')\n",
    "\n",
    "# 4. Number of adjectives\n",
    "cus_fn_mean_adj = get_mean_pos(test.review, Y_test, cus_wrong_idx, 1, 'ADJ')\n",
    "cus_fp_mean_adj = get_mean_pos(test.review, Y_test, cus_wrong_idx, -1, 'ADJ')\n",
    "cus_tn_mean_adj = get_mean_pos(test.review, Y_test, cus_correct_idx, -1, 'ADJ')\n",
    "cus_tp_mean_adj = get_mean_pos(test.review, Y_test, cus_correct_idx, 1, 'ADJ')\n",
    "\n",
    "# 5. Number of words trimmed\n",
    "cus_fn_mean_trim = get_mean_trim(test.review, X_test, Y_test, cus_wrong_idx, 1)\n",
    "cus_fp_mean_trim = get_mean_trim(test.review, X_test, Y_test, cus_wrong_idx, -1)\n",
    "cus_tn_mean_trim = get_mean_trim(test.review, X_test, Y_test, cus_correct_idx, -1)\n",
    "cus_tp_mean_trim = get_mean_trim(test.review, X_test, Y_test, cus_correct_idx, 1)\n",
    "\n",
    "# Structure: FP, TP, FN, TN\n",
    "labels = ['False Positive', 'True Positive', 'False Negative', 'True Negative']\n",
    "cus_analysis = {'Punctuations': [cus_fp_mean_punc, cus_tp_mean_punc, cus_fn_mean_punc, cus_tn_mean_punc],\n",
    "            'Adjectives': [cus_fp_mean_adj, cus_tp_mean_adj, cus_fn_mean_adj, cus_tn_mean_adj],\n",
    "            'Negation Words': [cus_fp_mean_neg, cus_tp_mean_neg, cus_fn_mean_neg, cus_tn_mean_neg],\n",
    "            'Tokens': [cus_fp_mean_len, cus_tp_mean_len, cus_fn_mean_len, cus_tn_mean_len],\n",
    "            'Words Trimmed':  [cus_fp_mean_trim, cus_tp_mean_trim, cus_fn_mean_trim, cus_tn_mean_trim]\n",
    "}\n",
    "\n",
    "cus_analysis_df = pd.DataFrame(cus_analysis, index=labels)\n",
    "\n",
    "cus_analysis_df.style.background_gradient(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=BoostingClassifier(n_clf=5000,  max_depth=1, criterion=\"entropy\", splitter='best')\n",
    "clf.fit(X_train,Y_train)\n",
    "y_pred= clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(Y_test,y_pred)\n",
    "print(\"Accuracy :\",acc)\n",
    "classificationReport = classification_report(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc08171",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_pred = y_pred\n",
    "\n",
    "ent_wrong_idx = [w for w in range(len(ent_pred)) if ent_pred[w] != Y_test[w]]\n",
    "ent_correct_idx = [w for w in range(len(ent_pred)) if ent_pred[w] == Y_test[w]]\n",
    "\n",
    "ent_raw_wrong_samples = [test.review[i] for i in ent_wrong_idx]\n",
    "ent_raw_correct_samples = [test.review[i] for i in ent_correct_idx]\n",
    "\n",
    "# 1. Number of tokens\n",
    "ent_fn_mean_len = get_mean_len(X_test, Y_test, ent_wrong_idx, 1)\n",
    "ent_fp_mean_len = get_mean_len(X_test, Y_test, ent_wrong_idx, -1)\n",
    "ent_tn_mean_len = get_mean_len(X_test, Y_test, ent_correct_idx, -1)\n",
    "ent_tp_mean_len = get_mean_len(X_test, Y_test, ent_correct_idx, 1)\n",
    "\n",
    "# 2. Number of punctuations\n",
    "ent_fn_mean_punc = get_mean_punc(test.review, Y_test, ent_wrong_idx, 1)\n",
    "ent_fp_mean_punc = get_mean_punc(test.review, Y_test, ent_wrong_idx, -1)\n",
    "ent_tn_mean_punc = get_mean_punc(test.review, Y_test, ent_correct_idx, -1)\n",
    "ent_tp_mean_punc = get_mean_punc(test.review, Y_test, ent_correct_idx, 1)\n",
    "\n",
    "# 3. Number of negation words\n",
    "ent_fn_mean_neg = get_mean_dep(test.review, Y_test, ent_wrong_idx, 1, 'neg')\n",
    "ent_fp_mean_neg = get_mean_dep(test.review, Y_test, ent_wrong_idx, -1, 'neg')\n",
    "ent_tn_mean_neg = get_mean_dep(test.review, Y_test, ent_correct_idx, -1, 'neg')\n",
    "ent_tp_mean_neg = get_mean_dep(test.review, Y_test, ent_correct_idx, 1, 'neg')\n",
    "\n",
    "# 4. Number of adjectives\n",
    "ent_fn_mean_adj = get_mean_pos(test.review, Y_test, ent_wrong_idx, 1, 'ADJ')\n",
    "ent_fp_mean_adj = get_mean_pos(test.review, Y_test, ent_wrong_idx, -1, 'ADJ')\n",
    "ent_tn_mean_adj = get_mean_pos(test.review, Y_test, ent_correct_idx, -1, 'ADJ')\n",
    "ent_tp_mean_adj = get_mean_pos(test.review, Y_test, ent_correct_idx, 1, 'ADJ')\n",
    "\n",
    "# 5. Number of words trimmed\n",
    "ent_fn_mean_trim = get_mean_trim(test.review, X_test, Y_test, ent_wrong_idx, 1)\n",
    "ent_fp_mean_trim = get_mean_trim(test.review, X_test, Y_test, ent_wrong_idx, -1)\n",
    "ent_tn_mean_trim = get_mean_trim(test.review, X_test, Y_test, ent_correct_idx, -1)\n",
    "ent_tp_mean_trim = get_mean_trim(test.review, X_test, Y_test, ent_correct_idx, 1)\n",
    "\n",
    "# Structure: FP, TP, FN, TN\n",
    "labels = ['False Positive', 'True Positive', 'False Negative', 'True Negative']\n",
    "ent_analysis = {'Punctuations': [ent_fp_mean_punc, ent_tp_mean_punc, ent_fn_mean_punc, ent_tn_mean_punc],\n",
    "            'Adjectives': [ent_fp_mean_adj, ent_tp_mean_adj, ent_fn_mean_adj, ent_tn_mean_adj],\n",
    "            'Negation Words': [ent_fp_mean_neg, ent_tp_mean_neg, ent_fn_mean_neg, ent_tn_mean_neg],\n",
    "            'Tokens': [ent_fp_mean_len, ent_tp_mean_len, ent_fn_mean_len, ent_tn_mean_len],\n",
    "            'Words Trimmed':  [ent_fp_mean_trim, ent_tp_mean_trim, ent_fn_mean_trim, ent_tn_mean_trim]\n",
    "}\n",
    "\n",
    "ent_analysis_df = pd.DataFrame(ent_analysis, index=labels)\n",
    "\n",
    "ent_analysis_df.style.background_gradient(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af11c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for punctuations\n",
    "criteria = ['Entropy','Gini']\n",
    "\n",
    "characteristics = 'Punctuations'\n",
    "\n",
    "punctuations = {'False Positive': [ent_analysis_df[characteristics]['False Positive'],\n",
    "                          cus_analysis_df[characteristics]['False Positive']],\n",
    "        'False Negative': [ent_analysis_df[characteristics]['False Negative'],\n",
    "                          cus_analysis_df[characteristics]['False Negative']],\n",
    "        'True Negative': [ent_analysis_df[characteristics]['True Negative'],\n",
    "                          cus_analysis_df[characteristics]['True Negative']],\n",
    "        'True Positive': [ent_analysis_df[characteristics]['True Positive'],\n",
    "                          cus_analysis_df[characteristics]['True Positive']],\n",
    "}\n",
    "\n",
    "punctuations_df = pd.DataFrame(punctuations, index = criteria)\n",
    "punc_plot = plt.figure()\n",
    "punctuations_df.plot(xlabel='Kernel', \n",
    "                     ylabel='Average punctuation %')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3791431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for adjectives\n",
    "criteria = ['Entropy','Gini']\n",
    "\n",
    "characteristics = 'Adjectives'\n",
    "\n",
    "adj = {'False Positive': [ent_analysis_df[characteristics]['False Positive'],\n",
    "                          cus_analysis_df[characteristics]['False Positive']],\n",
    "        'False Negative': [ent_analysis_df[characteristics]['False Negative'],\n",
    "                          cus_analysis_df[characteristics]['False Negative']],\n",
    "        'True Negative': [ent_analysis_df[characteristics]['True Negative'],\n",
    "                          cus_analysis_df[characteristics]['True Negative']],\n",
    "        'True Positive': [ent_analysis_df[characteristics]['True Positive'],\n",
    "                          cus_analysis_df[characteristics]['True Positive']],\n",
    "}\n",
    "\n",
    "adj_df = pd.DataFrame(adj, index = criteria)\n",
    "adj_plt = plt.figure()\n",
    "adj_df.plot(xlabel='Criteria', \n",
    "            ylabel='Average adjective %')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for negative words\n",
    "criteria = ['Entropy','Gini']\n",
    "\n",
    "characteristics = 'Negation Words'\n",
    "\n",
    "neg = {'False Positive': [ent_analysis_df[characteristics]['False Positive'],\n",
    "                          cus_analysis_df[characteristics]['False Positive']],\n",
    "        'False Negative': [ent_analysis_df[characteristics]['False Negative'],\n",
    "                          cus_analysis_df[characteristics]['False Negative']],\n",
    "        'True Negative': [ent_analysis_df[characteristics]['True Negative'],\n",
    "                          cus_analysis_df[characteristics]['True Negative']],\n",
    "        'True Positive': [ent_analysis_df[characteristics]['True Positive'],\n",
    "                          cus_analysis_df[characteristics]['True Positive']],\n",
    "}\n",
    "\n",
    "neg_df = pd.DataFrame(neg, index = criteria)\n",
    "neg_plt = plt.figure()\n",
    "neg_df.plot(xlabel='Criteria', \n",
    "            ylabel='Average negation %')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for average length\n",
    "criteria = ['Entropy','Gini']\n",
    "\n",
    "characteristics = 'Tokens'\n",
    "\n",
    "length = {'False Positive': [ent_analysis_df[characteristics]['False Positive'],\n",
    "                          cus_analysis_df[characteristics]['False Positive']],\n",
    "        'False Negative': [ent_analysis_df[characteristics]['False Negative'],\n",
    "                          cus_analysis_df[characteristics]['False Negative']],\n",
    "        'True Negative': [ent_analysis_df[characteristics]['True Negative'],\n",
    "                          cus_analysis_df[characteristics]['True Negative']],\n",
    "        'True Positive': [ent_analysis_df[characteristics]['True Positive'],\n",
    "                          cus_analysis_df[characteristics]['True Positive']],\n",
    "}\n",
    "\n",
    "length_df = pd.DataFrame(length, index = criteria)\n",
    "length_plt = plt.figure()\n",
    "length_df.plot(xlabel='Criteria', \n",
    "            ylabel='Average length')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for words trimmed\n",
    "criteria = ['Entropy','Gini']\n",
    "\n",
    "characteristics = 'Words Trimmed'\n",
    "\n",
    "trim = {'False Positive': [ent_analysis_df[characteristics]['False Positive'],\n",
    "                          cus_analysis_df[characteristics]['False Positive']],\n",
    "        'False Negative': [ent_analysis_df[characteristics]['False Negative'],\n",
    "                          cus_analysis_df[characteristics]['False Negative']],\n",
    "        'True Negative': [ent_analysis_df[characteristics]['True Negative'],\n",
    "                          cus_analysis_df[characteristics]['True Negative']],\n",
    "        'True Positive': [ent_analysis_df[characteristics]['True Positive'],\n",
    "                          cus_analysis_df[characteristics]['True Positive']],\n",
    "}\n",
    "\n",
    "trim_df = pd.DataFrame(trim, index = criteria)\n",
    "trim_plt = plt.figure()\n",
    "trim_df.plot(xlabel='Criteria', \n",
    "            ylabel='Average % words trimmed')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
