{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {
    "id": "44281064"
   },
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {
    "id": "221ffe46"
   },
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {
    "id": "7385ce53"
   },
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5J-0BOGIdRTi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16032,
     "status": "ok",
     "timestamp": 1651103881336,
     "user": {
      "displayName": "lo longting",
      "userId": "13591394878054317958"
     },
     "user_tz": -60
    },
    "id": "5J-0BOGIdRTi",
    "outputId": "4f8b9a43-11f5-4b8b-adb0-47b33d91d8fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[K     |████████████████████████████████| 287 kB 8.8 MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 49.0 MB/s \n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n",
      "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install unidecode\n",
    "!pip3 install contractions\n",
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ac481e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2020,
     "status": "ok",
     "timestamp": 1651103909026,
     "user": {
      "displayName": "lo longting",
      "userId": "13591394878054317958"
     },
     "user_tz": -60
    },
    "id": "e0ac481e",
    "outputId": "2b0053d1-0cbc-44b5-cb6a-da0a014293e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import unidecode\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import contractions\n",
    "import string\n",
    "from tqdm.notebook import tqdm # for showing progress bar\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# initialization\n",
    "pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "punc_translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "num_translator = str.maketrans(string.digits, ' ' * len(string.digits))\n",
    "nlp = en_core_web_sm.load()\n",
    "stopwords = stopwords.words('english')\n",
    "custom_negation = ['rather', 'instead']\n",
    "    \n",
    "def pre_processing(dataset):\n",
    "    \n",
    "    to_return = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        filtered_data = []\n",
    "        \n",
    "        # (1) remove html tags\n",
    "        dataset[i] = BeautifulSoup(dataset[i]).get_text()\n",
    "        \n",
    "        # (2) remove urls\n",
    "        dataset[i] = re.sub(r'http\\S+', '', dataset[i])\n",
    "        dataset[i] = re.sub(r'www\\S+', '', dataset[i])\n",
    "        \n",
    "        # (3) remove email addresses\n",
    "        dataset[i] = re.sub(r'\\S*@\\S*\\s?', '', dataset[i])\n",
    "        \n",
    "        # (3) convert to lower case\n",
    "        dataset[i] = dataset[i].casefold()\n",
    "        \n",
    "        # (4) convert accented character\n",
    "        dataset[i] = unidecode.unidecode(dataset[i]) \n",
    "        \n",
    "        # (5) if there are >2 consecutive duplicated characters, convert to 2 consecutive duplicated characters\n",
    "        # e.g. finallllly --> finally\n",
    "        dataset[i] = pattern.sub(r\"\\1\\1\", dataset[i]) \n",
    "        \n",
    "        # (6) expand contractions\n",
    "        dataset[i] = contractions.fix(dataset[i])\n",
    "        \n",
    "        # (7) replace punctuation with space\n",
    "        dataset[i] = dataset[i].translate(punc_translator)\n",
    "        \n",
    "        # (8) replace numbers with space\n",
    "        dataset[i] = dataset[i].translate(num_translator)\n",
    "        \n",
    "        # (9) spacy tokenization\n",
    "        tokens = nlp(dataset[i])\n",
    "            \n",
    "        for token in tokens:\n",
    "            \n",
    "            # Lemmatisation\n",
    "            word = token.lemma_\n",
    "            \n",
    "            # filter out words that are:\n",
    "            # - stopwords\n",
    "            # - with length <= 2\n",
    "            # - demonstratives (e.g. this, that, those)\n",
    "            # - pronoun and proper nouns (e.g. names)\n",
    "            # - spaces\n",
    "            \n",
    "            names = [ent.text for ent in tokens if ent.ent_type_]\n",
    "            \n",
    "            if (word != \"-PRON-\") and (word !=\"-PROPN-\") and (word not in names) and (not token.is_space):\n",
    "               \n",
    "                #print(word)\n",
    "\n",
    "                if (token.dep_ == 'neg') or (word in custom_negation):\n",
    "                    filtered_data.append('_NEG_')\n",
    "                    continue\n",
    "                \n",
    "                # remove the word \"like\" when it is used as preposition\n",
    "                if (word == 'like' and token.dep_ == 'prep'):\n",
    "                    continue\n",
    "                \n",
    "                # remove stopwords\n",
    "                if (word in stopwords):\n",
    "                    continue\n",
    "\n",
    "                # remove words with len <= 2\n",
    "                elif (len(word) <= 2):\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    filtered_data.append(word)\n",
    "        \n",
    "        # join words\n",
    "        filtered_data = ' '.join(filtered_data)\n",
    "        \n",
    "        # Negation tagging\n",
    "        filtered_data = re.sub(r'_NEG_\\s', '_NEG_', filtered_data)\n",
    "        filtered_data = re.sub(r\"(_NEG_)\\1{1,}\", '_NEG_', filtered_data) # remove duplicated negation tagging\n",
    "        \n",
    "        to_return.append(filtered_data)\n",
    "        \n",
    "    return to_return\n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    \n",
    "    # Read the CSV files for training and test sets\n",
    "    train = pd.read_csv(train_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = np.array(train.review)\n",
    "    X_test = np.array(test.review)\n",
    "    \n",
    "    y_train = np.array(train.sentiment)\n",
    "    y_train[y_train=='positive'] = 1\n",
    "    y_train[y_train=='negative'] = -1\n",
    "    y_train = y_train.astype('int')\n",
    "    \n",
    "    y_test = np.array(test.sentiment)\n",
    "    y_test[y_test=='positive'] = 1\n",
    "    y_test[y_test=='negative'] = -1\n",
    "    y_test = y_test.astype('int')\n",
    "    \n",
    "    # Extract bag of words features\n",
    "    print(\"Train set: \")\n",
    "    print(\"Preprocessing progress: \")\n",
    "    X_train = pre_processing(X_train) \n",
    "    print(\"--Done--\\n\")\n",
    "    print(\"Test set: \")\n",
    "    print(\"Preprocessing progress: \")\n",
    "    X_test = pre_processing(X_test)\n",
    "    print('--Done--')\n",
    "    \n",
    "    return (X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4111e27",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1651103806126,
     "user": {
      "displayName": "lo longting",
      "userId": "13591394878054317958"
     },
     "user_tz": -60
    },
    "id": "f4111e27"
   },
   "outputs": [],
   "source": [
    "# self testing code - remove before submission\n",
    "(X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc3af27",
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1651103913316,
     "user": {
      "displayName": "lo longting",
      "userId": "13591394878054317958"
     },
     "user_tz": -60
    },
    "id": "7fc3af27"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from collections import defaultdict\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, kernel='rbf', C=1.6058997806999291, gamma=0.8101577349324269):\n",
    "        \n",
    "        #implement initialisation\n",
    "        self.clf = svm.SVC()\n",
    "        self.kernel = kernel\n",
    "        \n",
    "        # regularization parameter\n",
    "        self.C = C # penalty parameter\n",
    "        \n",
    "        # kernel parameters\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(min_df = 2, # remove words that appear too rarely\n",
    "                                          max_df = 0.7, # remove words that appear too often\n",
    "                                          ngram_range=(1,5), # 1-2 gram\n",
    "                                          max_features=30000,\n",
    "                                          smooth_idf = True, # +1 to all frequencies, prevent division by zero\n",
    "                                          sublinear_tf = True #use log for TF, clip extreme values\n",
    "                                          )\n",
    "        \n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "  \n",
    "    def custom_kernel(self, X, y):\n",
    "        \n",
    "        print('Computing custom kernel...')\n",
    "        \n",
    "        # 1. Histogram intersection kernel （0.86）\n",
    "        # ---------------- BEGIN ----------------#\n",
    "        kernel = np.zeros((X.shape[0], y.shape[0]))\n",
    "\n",
    "        for d in tqdm(range(X.shape[1])):\n",
    "            column_1 = X[:, d].reshape(-1, 1)\n",
    "            column_2 = y[:, d].reshape(-1, 1)\n",
    "            kernel += np.minimum(column_1, column_2.T)\n",
    "\n",
    "        # ------------------ END -----------------#\n",
    "        \n",
    "        return kernel\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "\n",
    "        # Transform data into tfidf feature vectors\n",
    "        X = self.vectorizer.fit_transform(X)\n",
    "\n",
    "        # calling diff kernels\n",
    "        if self.kernel == 'linear':\n",
    "            self.clf = svm.SVC(kernel='linear', C=self.C)\n",
    "\n",
    "        elif self.kernel == 'poly':\n",
    "            self.clf = svm.SVC(kernel='poly', C=self.C, degree=self.d)\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            # for hyperparameter tuning\n",
    "            self.clf = svm.SVC(kernel='rbf', C=self.C, gamma=self.gamma)\n",
    "\n",
    "        elif self.kernel == 'custom':\n",
    "            self.clf = svm.SVC(kernel=self.custom_kernel, C=self.C)\n",
    "        \n",
    "        self.clf.fit(X,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # prediction routine for the SVM\n",
    "        X = self.vectorizer.transform(X)\n",
    "        \n",
    "        return self.clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {
    "id": "35e6f272"
   },
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89603f43",
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1651103918892,
     "user": {
      "displayName": "lo longting",
      "userId": "13591394878054317958"
     },
     "user_tz": -60
    },
    "id": "89603f43"
   },
   "outputs": [],
   "source": [
    "def test_func_svm(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ffd4adf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "error",
     "timestamp": 1651103921517,
     "user": {
      "displayName": "lo longting",
      "userId": "13591394878054317958"
     },
     "user_tz": -60
    },
    "id": "4ffd4adf",
    "outputId": "a240432a-90d9-4499-a2c5-3121891d8938",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1f5baa16f6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_func_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movie_review_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"movie_review_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-092567975f27>\u001b[0m in \u001b[0;36mtest_func_svm\u001b[0;34m(dataset_train, dataset_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_func_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_bag_of_words_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e67ba66fa1c5>\u001b[0m in \u001b[0;36mextract_bag_of_words_train_test\u001b[0;34m(train_file, test_file)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Read the CSV files for training and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movie_review_train.csv'"
     ]
    }
   ],
   "source": [
    "acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {
    "id": "61056292"
   },
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805e672",
   "metadata": {
    "id": "3805e672"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self,n_clf=100, max_depth=None, criterion=None, splitter=None):\n",
    "        \n",
    "        # Hyperparameter for AdaBoost\n",
    "        self.n_clf=n_clf\n",
    "        \n",
    "        # Hyperparameters for decision tree\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        \n",
    "        # TF-IDF vectorizer to convert the feature vectors\n",
    "        self.tf_idf = TfidfVectorizer(min_df = 2, # remove words that appear too rarely\n",
    "                                      max_df = 0.7, # remove words that appear too often\n",
    "                                      sublinear_tf = True,\n",
    "                                      ngram_range=(1,5),\n",
    "                                      max_features=30000,\n",
    "                                      smooth_idf = True\n",
    "                                      )\n",
    "        \n",
    "\n",
    "\n",
    "    def update_w(self, w, al, y, pred):\n",
    "        return w * np.exp(al * (np.not_equal(y, pred)))\n",
    "    \n",
    "    def calc_err(self, y, pred, w):       \n",
    "        return sum(w * np.not_equal(y, pred))/sum(w)\n",
    "    \n",
    "    def calc_alph(self, err):\n",
    "        eps = 1e-10\n",
    "        return np.log((1 - err) / (err + eps))\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        n_samples = len(X)\n",
    "        self.clfs=[]\n",
    "        self.alpha=[]\n",
    "\n",
    "        X = self.tf_idf.fit_transform(X)\n",
    "        \n",
    "        for m in tqdm(range(self.n_clf)):\n",
    "            \n",
    "            if m == 0:\n",
    "                # init weights\n",
    "                w = np.full(n_samples,(1/n_samples))\n",
    "            else:\n",
    "                # update weights\n",
    "                w = self.update_w(w, alph, y, pred)\n",
    "            \n",
    "            clf = DecisionTreeClassifier(max_depth = self.max_depth, \n",
    "                                         criterion = self.criterion,\n",
    "                                         splitter = self.splitter\n",
    "                                        )\n",
    "\n",
    "            clf = clf.fit(X, y, sample_weight = w)\n",
    "            \n",
    "            pred = clf.predict(X) # predictions made by the weak classifier\n",
    "            \n",
    "            # save classifier\n",
    "            self.clfs.append(clf)\n",
    "            \n",
    "            # calculate error\n",
    "            err = self.calc_err(y, pred, w)\n",
    "            \n",
    "            # cal alph \n",
    "            alph = self.calc_alph(err)\n",
    "            self.alpha.append(alph)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # init df for storing pred from each weak classifier (decision tree)\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.n_clf))\n",
    "        \n",
    "        X = self.tf_idf.transform(X)    \n",
    "        \n",
    "        for m in tqdm(range(self.n_clf)):\n",
    "            pred_m = self.clfs[m].predict(X) * self.alpha[m]\n",
    "            weak_preds.iloc[:,m] = pred_m\n",
    "\n",
    "        # Calculate final predictions\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155fc0d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b894d57453cd470594e08a592e548d40",
      "07b69fe04bf747caae20d4414ad7071c"
     ]
    },
    "id": "0155fc0d",
    "outputId": "3d9acd77-0273-4cb9-dad2-4b1cc8f9b982"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b894d57453cd470594e08a592e548d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b69fe04bf747caae20d4414ad7071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.84      0.84       731\n",
      "           1       0.85      0.85      0.85       769\n",
      "\n",
      "    accuracy                           0.85      1500\n",
      "   macro avg       0.85      0.85      0.85      1500\n",
      "weighted avg       0.85      0.85      0.85      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "start = datetime.now()\n",
    "clf=BoostingClassifier(n_clf=10000,  max_depth=1, criterion='gini', splitter='best')\n",
    "clf.fit(X_train,Y_train)\n",
    "y_pred= clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(Y_test,y_pred)\n",
    "print(\"Accuracy :\",acc)\n",
    "classificationReport = classification_report(Y_test, y_pred)\n",
    "print(classificationReport)\n",
    "end = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab1729",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "43088fec9fc94ca1a16584e55423d1de",
      "9f6b6024906943ec921a2622258332cc"
     ]
    },
    "id": "71ab1729",
    "outputId": "84d58a35-1115-4e03-d333-68cd974ce072"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43088fec9fc94ca1a16584e55423d1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6b6024906943ec921a2622258332cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.84      0.84       731\n",
      "           1       0.85      0.85      0.85       769\n",
      "\n",
      "    accuracy                           0.85      1500\n",
      "   macro avg       0.85      0.85      0.85      1500\n",
      "weighted avg       0.85      0.85      0.85      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "start = datetime.now()\n",
    "clf=BoostingClassifier(n_clf=10000,  max_depth=1, criterion='gini', splitter='best')\n",
    "clf.fit(X_train,Y_train)\n",
    "y_pred= clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(Y_test,y_pred)\n",
    "print(\"Accuracy :\",acc)\n",
    "classificationReport = classification_report(Y_test, y_pred)\n",
    "print(classificationReport)\n",
    "end = datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {
    "id": "af6e0987"
   },
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632591c",
   "metadata": {
    "id": "4632591c"
   },
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    Y_Pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c27de1",
   "metadata": {
    "id": "d6c27de1"
   },
   "outputs": [],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Coursework2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
