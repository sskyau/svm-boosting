{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Sayuri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/Sayuri/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/Sayuri/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "\n",
    "def pre_processing(dataset):\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        #print(i)\n",
    "        # remove html tags\n",
    "        dataset[i] = BeautifulSoup(dataset[i]).get_text()\n",
    "        \n",
    "        # convert to lower case\n",
    "        dataset[i] = dataset[i].lower()\n",
    "        \n",
    "        # tokenize\n",
    "        dataset[i] = word_tokenize(dataset[i])\n",
    "                \n",
    "        # remove punctuation\n",
    "        dataset[i] = [word for word in dataset[i] if word.isalpha()]\n",
    "        \n",
    "        # remove lengthened words e.g. finallllly\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "        dataset[i] = [pattern.sub(r\"\\1\\1\", word) for word in dataset[i]]\n",
    "        \n",
    "        # remove stop words\n",
    "        for word in dataset[i]:\n",
    "            if word in stopwords.words('english'):\n",
    "                dataset[i].remove(word)\n",
    "        \n",
    "        # stemming\n",
    "        stemmer = PorterStemmer()\n",
    "        dataset[i] = [stemmer.stem(word) for word in dataset[i]]\n",
    "        \n",
    "        # join words with space\n",
    "        dataset[i] = ' '.join(dataset[i])\n",
    "             \n",
    "    return dataset\n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    \n",
    "    # Read the CSV files for training and test sets\n",
    "    train = pd.read_csv(train_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = np.array(train.review)\n",
    "    X_test = np.array(test.review)\n",
    "    y_train = np.array(train.sentiment)\n",
    "    y_test = np.array(test.sentiment)\n",
    "    \n",
    "    # Extract bag of words features\n",
    "    X_train = pre_processing(X_train) \n",
    "    print('Train set: done processing data')\n",
    "    X_test = pre_processing(X_test)\n",
    "    print('Test set: done processing data')\n",
    "    \n",
    "    return (X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e94c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score \n",
    "from warnings import warn\n",
    "from grakel.kernels import ShortestPath\n",
    "from nltk.tokenize import word_tokenize\n",
    "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, kernel, C=None, gamma=None, coef=None, d=None):\n",
    "        \n",
    "        #implement initialisation\n",
    "        self.kernel = kernel\n",
    "        self.tf_idf = None\n",
    "        self.graph_kernel = None\n",
    "        \n",
    "        # for cross-validation \n",
    "        ## to be implemented \n",
    "        self.train_val_split = 0.8\n",
    "        self.num_fold = 5\n",
    "        \n",
    "        # regularization parameter\n",
    "        self.C = C # penalty parameter\n",
    "        \n",
    "        # kernel parameters\n",
    "        self.gamma = gamma # kernel coef for rbf, poly, sigmoid\n",
    "        self.coef = coef #independent term for polynomial and sigmoid kernels\n",
    "        self.d = d # degree; for polynomial kernel\n",
    "\n",
    "        \n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "    \n",
    "    def custom_graph_kernel(self, train, val): #to be implemented\n",
    "        # reference: https://www.jmlr.org/papers/volume21/18-370/18-370.pdf\n",
    "        \n",
    "        self.graph_kernel = ShortestPath()\n",
    "        G_train = self.graph_kernel.fit_transform(train)\n",
    "        G_val = self.graph_kernel.transform(val)\n",
    "        \n",
    "        return G_train, G_val\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "        \n",
    "        # cross validation\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=self.num_fold, shuffle=True, random_state=1)\n",
    "        fold_acc_lst = []\n",
    "  \n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "        \n",
    "            x_train_fold, x_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # get feature vectors for train and val sets\n",
    "            if self.kernel != 'custom':\n",
    "                #print('not custom')\n",
    "                self.tf_idf = TfidfVectorizer(min_df = 5,\n",
    "                                              max_df = 0.8,\n",
    "                                              sublinear_tf = True)\n",
    "                \n",
    "                x_train_fold = self.tf_idf.fit_transform(x_train_fold)\n",
    "                x_val_fold = self.tf_idf.transform(x_val_fold)\n",
    "\n",
    "                # calling diff kernels\n",
    "                if self.kernel == 'linear':\n",
    "                    #print('linear')\n",
    "                    self.clf = svm.SVC(kernel='linear', C=self.C)\n",
    "\n",
    "                elif self.kernel == 'polynomial':\n",
    "                    self.clf = svm.SVC(kernel='poly', C=self.C, gamma=self.gamma, coef0=self.coef, degree=self.d)\n",
    "\n",
    "                elif self.kernel == 'sigmoid':\n",
    "                    self.clf = svm.SVC(kernel='sigmoid', C=self.C, gamma=self.gamma, coef0=self.coef)\n",
    "\n",
    "                elif self.kernel == 'rbf':\n",
    "                    self.clf = svm.SVC(kernel='rbf', C=self.C, gamma=self.gamma, coef0=self.coef)\n",
    "            \n",
    "            elif self.kernel == 'custom':\n",
    "                x_train_fold = [word_tokenize(w) for w in x_train_fold]\n",
    "                #print(x_train_fold)\n",
    "                G_train, G_test = self.custom_graph_kernel(x_train_fold, x_val_fold)\n",
    "                self.clf = svm.SVC(kernel='precomputed', C=self.C)\n",
    "\n",
    "            self.clf.fit(x_train_fold, y_train_fold)\n",
    "            y_pred_fold = self.clf.predict(x_val_fold)\n",
    "            fold_acc_lst.append(accuracy_score(y_val_fold, y_pred_fold))\n",
    "            \n",
    "        return fold_acc_lst\n",
    "        \n",
    "        \n",
    "        # for submission\n",
    "        #self.td_idf = TfidfVectorizer()\n",
    "        #X = self.tf_idf.fit_transform(X)\n",
    "        #return self.clf.fit(X, y) \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        if self.kernel == 'custom':\n",
    "            X = self.graph_kernel.transform(X)\n",
    "        else:\n",
    "            # get tfidf feature vector\n",
    "            X = self.tf_idf.transform(X)\n",
    "            \n",
    "        # prediction routine for the SVM\n",
    "        return self.clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db3e162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: done processing data\n",
      "Test set: done processing data\n"
     ]
    }
   ],
   "source": [
    "# self testing code - remove before submission\n",
    "(X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a582715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:  1\n",
      "c:  0.3\n",
      "[0.849 0.849 0.872 0.865 0.879]\n",
      "mean score:  0.8628\n",
      "Accuracy: 0.8713333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.86      0.87       731\n",
      "    positive       0.87      0.89      0.88       769\n",
      "\n",
      "    accuracy                           0.87      1500\n",
      "   macro avg       0.87      0.87      0.87      1500\n",
      "weighted avg       0.87      0.87      0.87      1500\n",
      "\n",
      "run:  2\n",
      "c:  0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "# self testing code - remove before submission\n",
    "# Hyperparameter tuning for linear kernel\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "linear_perf = []\n",
    "linear_rpt = []\n",
    "\n",
    "# for linear kernel\n",
    "C_rng = np.linspace(0.3,-0.4,10)\n",
    "\n",
    "run = 1\n",
    "\n",
    "for c in C_rng:\n",
    "    print('run: ', run)\n",
    "    print('c: ', c)\n",
    "\n",
    "    linear_sc = SVMClassifier(kernel='linear', C=c)\n",
    "    acc_list = np.array(linear_sc.fit(X_train, Y_train))\n",
    "    print(acc_list)\n",
    "    print('mean score: ', np.mean(acc_list))\n",
    "    \n",
    "    Y_Pred = linear_sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    print(classification_report(Y_test, Y_Pred))\n",
    "    linear_perf.append(acc_list)\n",
    "    linear_rpt.append(classification_report(Y_test, Y_Pred))\n",
    "    \n",
    "    run += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6df1b36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.851 0.857 0.872 0.872 0.876]\n",
      "mean score:  0.8656\n",
      "Accuracy: 0.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.86      0.87       731\n",
      "    positive       0.87      0.88      0.87       769\n",
      "\n",
      "    accuracy                           0.87      1500\n",
      "   macro avg       0.87      0.87      0.87      1500\n",
      "weighted avg       0.87      0.87      0.87      1500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nk1_rng = np.linspace(-2,2,5)\\nC_rng = [2**k for k in k1_rng]\\nk2_rng = np.linspace(-2,2,5)\\ngamma_rng = [2**k for k in k2_rng]\\nk3_rng = np.linspace(-1.2,2,5)\\ncoef_rng = [2**k for k in k3_rng]\\n\\nrun = 1\\n\\nfor c in C_rng:\\n    print(\\'run: \\', run)\\n    print(\\'c: \\', c)\\n    \\n    for gamma in gamma_rng:\\n        \\n        print(\\'gamma: \\', gamma)\\n        \\n        for coef in coef_rng:\\n            \\n            print(\\'coef: \\', coef)\\n\\n            sc = SVMClassifier(kernel=\\'rbf\\', C=c, gamma=gamma, coef=coef)\\n\\n            acc_list = np.array(sc.fit(X_train, Y_train))\\n            print(acc_list)\\n            print(\\'mean score: \\', np.mean(acc_list))\\n            Y_Pred = sc.predict(X_test)\\n            acc = accuracy_score(Y_test, Y_Pred)\\n            print(\"Accuracy:\",acc)\\n            print(classification_report(Y_test, Y_Pred))\\n            rbf_perf.append(acc_list)\\n            rbf_rpt.append(classification_report(Y_test, Y_Pred))\\n    \\n            run += 1\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self testing code - remove before submission\n",
    "## RBF kernel\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rbf_perf = []\n",
    "rbf_rpt = []\n",
    "\n",
    "# for rbf kernel\n",
    "sc = SVMClassifier(kernel='rbf', C=1, gamma='scale', coef=0)\n",
    "\n",
    "acc_list = np.array(sc.fit(X_train, Y_train))\n",
    "print(acc_list)\n",
    "print('mean score: ', np.mean(acc_list))\n",
    "Y_Pred = sc.predict(X_test)\n",
    "acc = accuracy_score(Y_test, Y_Pred)\n",
    "print(\"Accuracy:\",acc)\n",
    "print(classification_report(Y_test, Y_Pred))\n",
    "rbf_perf.append(acc_list)\n",
    "rbf_rpt.append(classification_report(Y_test, Y_Pred))\n",
    "\n",
    "# Hyperparameter tuning\n",
    "'''\n",
    "k1_rng = np.linspace(-2,2,5)\n",
    "C_rng = [2**k for k in k1_rng]\n",
    "k2_rng = np.linspace(-2,2,5)\n",
    "gamma_rng = [2**k for k in k2_rng]\n",
    "k3_rng = np.linspace(-1.2,2,5)\n",
    "coef_rng = [2**k for k in k3_rng]\n",
    "\n",
    "run = 1\n",
    "\n",
    "for c in C_rng:\n",
    "    print('run: ', run)\n",
    "    print('c: ', c)\n",
    "    \n",
    "    for gamma in gamma_rng:\n",
    "        \n",
    "        print('gamma: ', gamma)\n",
    "        \n",
    "        for coef in coef_rng:\n",
    "            \n",
    "            print('coef: ', coef)\n",
    "\n",
    "            sc = SVMClassifier(kernel='rbf', C=c, gamma=gamma, coef=coef)\n",
    "\n",
    "            acc_list = np.array(sc.fit(X_train, Y_train))\n",
    "            print(acc_list)\n",
    "            print('mean score: ', np.mean(acc_list))\n",
    "            Y_Pred = sc.predict(X_test)\n",
    "            acc = accuracy_score(Y_test, Y_Pred)\n",
    "            print(\"Accuracy:\",acc)\n",
    "            print(classification_report(Y_test, Y_Pred))\n",
    "            rbf_perf.append(acc_list)\n",
    "            rbf_rpt.append(classification_report(Y_test, Y_Pred))\n",
    "    \n",
    "            run += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5091b0fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not custom\n",
      "not custom\n",
      "not custom\n",
      "not custom\n",
      "not custom\n",
      "[0.846 0.852 0.862 0.858 0.867]\n",
      "mean score:  0.857\n",
      "['positive' 'positive' 'positive' ... 'positive' 'positive' 'negative']   ['negative' 'positive' 'positive' ... 'negative' 'positive' 'negative']\n",
      "Accuracy: 0.8586666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.85      0.85       731\n",
      "    positive       0.86      0.86      0.86       769\n",
      "\n",
      "    accuracy                           0.86      1500\n",
      "   macro avg       0.86      0.86      0.86      1500\n",
      "weighted avg       0.86      0.86      0.86      1500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrun = 1\\n\\nfor d in d_rng:\\n    print(\\'dimension: \\', d)\\n    \\n    for c in C_rng:\\n        print(\\'c: \\', c)\\n        \\n        for gamma in gamma_rng:\\n            print(\\'gamma: \\', gamma)\\n            \\n            for coef in coef_rng:\\n                print(\\'coef: \\', coef)\\n\\n                sc = SVMClassifier(kernel=\\'polynomial\\', C=c, gamma=gamma, d=d, coef=coef)\\n\\n                acc_list = np.array(sc.fit(X_train, Y_train))\\n                print(acc_list)\\n                print(\\'mean score: \\', np.mean(acc_list))\\n                Y_Pred = sc.predict(X_test)\\n                print(Y_test, \\' \\', Y_Pred)\\n                acc = accuracy_score(Y_test, Y_Pred)\\n                print(\"Accuracy:\",acc)\\n                print(classification_report(Y_test, Y_Pred))\\n                poly_perf.append(acc_list)\\n                poly_rpt.append(classification_report(Y_test, Y_Pred))\\n\\n                run += 1\\n                \\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self testing code - remove before submission\n",
    "# Polynomial kernel\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "poly_perf = []\n",
    "poly_rpt = []\n",
    "\n",
    "k1_rng = np.linspace(-5,5,5)\n",
    "C_rng = [2**k for k in k1_rng]\n",
    "k2_rng = np.linspace(-5,5,5)\n",
    "gamma_rng = [2**k for k in k2_rng]\n",
    "k3_rng = np.linspace(2,5,5)\n",
    "coef_rng = [2**k for k in k3_rng]\n",
    "d_rng = np.linspace(3,5,3)\n",
    "\n",
    "\n",
    "# for polynomial kernel\n",
    "sc = SVMClassifier(kernel='polynomial', C=c, gamma=1, d=3, coef=2)\n",
    "\n",
    "acc_list = np.array(sc.fit(X_train, Y_train))\n",
    "print(acc_list)\n",
    "print('mean score: ', np.mean(acc_list))\n",
    "Y_Pred = sc.predict(X_test)\n",
    "print(Y_test, ' ', Y_Pred)\n",
    "acc = accuracy_score(Y_test, Y_Pred)\n",
    "print(\"Accuracy:\",acc)\n",
    "print(classification_report(Y_test, Y_Pred))\n",
    "poly_perf.append(acc_list)\n",
    "poly_rpt.append(classification_report(Y_test, Y_Pred))\n",
    "\n",
    "'''\n",
    "run = 1\n",
    "\n",
    "for d in d_rng:\n",
    "    print('dimension: ', d)\n",
    "    \n",
    "    for c in C_rng:\n",
    "        print('c: ', c)\n",
    "        \n",
    "        for gamma in gamma_rng:\n",
    "            print('gamma: ', gamma)\n",
    "            \n",
    "            for coef in coef_rng:\n",
    "                print('coef: ', coef)\n",
    "\n",
    "                sc = SVMClassifier(kernel='polynomial', C=c, gamma=gamma, d=d, coef=coef)\n",
    "\n",
    "                acc_list = np.array(sc.fit(X_train, Y_train))\n",
    "                print(acc_list)\n",
    "                print('mean score: ', np.mean(acc_list))\n",
    "                Y_Pred = sc.predict(X_test)\n",
    "                print(Y_test, ' ', Y_Pred)\n",
    "                acc = accuracy_score(Y_test, Y_Pred)\n",
    "                print(\"Accuracy:\",acc)\n",
    "                print(classification_report(Y_test, Y_Pred))\n",
    "                poly_perf.append(acc_list)\n",
    "                poly_rpt.append(classification_report(Y_test, Y_Pred))\n",
    "\n",
    "                run += 1\n",
    "                \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0f93252",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "each element of X must have at least one and at most 3 elements\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1c/pk64lqn53r13zfp4b78pl9rc0000gn/T/ipykernel_9088/1331240280.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# for polynomial kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'custom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1c/pk64lqn53r13zfp4b78pl9rc0000gn/T/ipykernel_9088/1463910972.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mx_train_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_train_fold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m#print(x_train_fold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mG_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_graph_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1c/pk64lqn53r13zfp4b78pl9rc0000gn/T/ipykernel_9088/1463910972.py\u001b[0m in \u001b[0;36mcustom_graph_kernel\u001b[0;34m(self, train, val)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShortestPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mG_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_kernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mG_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_kernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/grakel/kernels/shortest_path.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \"\"\"\n\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_method_calling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;31m# calculate feature matrices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/grakel/kernels/kernel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`fit` input cannot be None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Return the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/grakel/kernels/shortest_path.py\u001b[0m in \u001b[0;36mparse_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    462\u001b[0m                     \u001b[0mspm_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_shortest_path_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                     raise TypeError('each element of X must have at least' +\n\u001b[0m\u001b[1;32m    465\u001b[0m                                     ' one and at most 3 elements\\n')\n\u001b[1;32m    466\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: each element of X must have at least one and at most 3 elements\n"
     ]
    }
   ],
   "source": [
    "# self testing code - remove before submission\n",
    "# Hyperparameter tuning for polynomial kernel\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "custom_perf = []\n",
    "custom_rpt = []\n",
    "\n",
    "# for polynomial kernel\n",
    "sc = SVMClassifier(kernel='custom')\n",
    "acc_list = np.array(sc.fit(X_train, Y_train))\n",
    "print(acc_list)\n",
    "print('mean score: ', np.mean(acc_list))\n",
    "Y_Pred = sc.predict(X_test)\n",
    "print(Y_test, ' ', Y_Pred)\n",
    "acc = accuracy_score(Y_test, Y_Pred)\n",
    "print(\"Accuracy:\",acc)\n",
    "print(classification_report(Y_test, Y_Pred))\n",
    "poly_perf.append(acc_list)\n",
    "poly_rpt.append(classification_report(Y_test, Y_Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1585011c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "coef_ is only available when using a linear kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1c/pk64lqn53r13zfp4b78pl9rc0000gn/T/ipykernel_9088/13397734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mplot_coefficients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/1c/pk64lqn53r13zfp4b78pl9rc0000gn/T/ipykernel_9088/13397734.py\u001b[0m in \u001b[0;36mplot_coefficients\u001b[0;34m(classifier, feature_names, top_features)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_coefficients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtop_positive_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtop_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtop_negative_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mcoef_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coef_ is only available when using a linear kernel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_coef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: coef_ is only available when using a linear kernel"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    " \n",
    "    print(top_coefficients)\n",
    "    # create plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_coefficients(sc.clf, sc.tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57559f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "89603f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ffd4adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-1f5baa16f6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_func_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movie_review_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"movie_review_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-092567975f27>\u001b[0m in \u001b[0;36mtest_func_svm\u001b[0;34m(dataset_train, dataset_test)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_bag_of_words_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mY_Pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-d70aa0da6353>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Ensure you call your own defined kernel here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_kernel' is not defined"
     ]
    }
   ],
   "source": [
    "acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3805e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self):\n",
    "        import numpy as np\n",
    "        #implement initialisation\n",
    "        self.some_paramter=1\n",
    "    def fit(self, X,y):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        import numpy as np\n",
    "        #implement training of the boosting classifier\n",
    "        return \n",
    "    def predict(self, X):\n",
    "        # implement prediction of the boosting classifier\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4632591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    Y_Pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c27de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
